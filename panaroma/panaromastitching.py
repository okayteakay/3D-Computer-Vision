# -*- coding: utf-8 -*-
"""PanaromaStitching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wDxcAI5nFaG2BimVSvb9A8kOW8k3AIEZ
"""

!pip install opencv-python==3.4.2.17
!pip install opencv-contrib-python==3.4.2.17

import numpy as np 
import cv2 
from google.colab.patches import cv2_imshow
import imutils

def image_dimensions(image):
    #img = cv2.imread('image')
    (h, w) = image.shape[:2]
    return (h,w)

def points(first_image, second_image):
    (h1, w1) = image_dimensions(first_image)
    (h2, w2) = image_dimensions(second_image)
    join = np.zeros((max(h1, h2), w1 + w2, 3), dtype = 'uint8')
    join[0:h1, 0:w1] = first_image            #y
    join[0:h2, w1:] = second_image            #y
    return join

p = points(first_image, second_image)

p

p.shape

def detect_feature_keypoints(image):
  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  descriptors = cv2.xfeatures2d.SIFT_create()               #Works only with the specified versions of opencv-python & opencv-contrib. 
  (Keypoints, features) = descriptors.detectAndCompute(image, None)
  Keypoints = np.float32([i.pt for i in Keypoints])
  return (Keypoints, features)

def get_all_matches(features1,features2):
  match_instance = cv2.DescriptorMatcher_create("BruteForce")
  all_Matches = match_instance.knnMatch(features1,features2, 2)
  return all_Matches

def get_all_valid_matches(all_Matches, lowe_ratio):
  val = []     #Valid Matches
  for v in all_Matches:
    if len(v) == 2 and v[0].distance < v[1].distance * lowe_ratio:
      val.append((v[0].trainIdx, v[0].queryIdx))
  return val

def match_keypoints(Keypoints1, Keypoints2, features1, features2, lowe_ratio, max_threshold):
  All_Matches = get_all_matches(features1, features2)
  Valid_Matches = get_all_valid_matches(All_Matches, 0.75)

  if len(Valid_Matches)>4:

    points1= np.float32([Keypoints1[i] for (_,i) in Valid_Matches])
    points2 = np.float32([Keypoints2[i] for (i,_) in Valid_Matches])

    (Homography, status) = compute_homography(points1, points2, max_threshold)
    return (Valid_Matches, Homography, status)
  else:
    return None

def compute_homography(points1, points2, max_threshold):
  (H, status) = cv2.findHomography(points1, points2, cv2.RANSAC, max_threshold)
  return(H, status)

def draw_matches(image1, image2, Keypoints1, Keypoints2, matches, status):
  (h1, w1) = image_dimensions(image1)
  vis = points(image1, image2)

  for ((trainIdx, queryIdx), s) in zip(matches, status):
            if s == 1:
                pt1 = (int(Keypoints1[queryIdx][0]), int(Keypoints1[queryIdx][1]))
                pt2 = (int(Keypoints2[trainIdx][0]) + w1, int(Keypoints2[trainIdx][1]))
                cv2.line(vis, pt1, pt2, (0, 255, 0), 1)

  return vis

def get_warp_perspective(image1,image2,Homography):
        val = image1.shape[1] + image2.shape[1]
        result_image = cv2.warpPerspective(image1, Homography, (val , image1.shape[0]))

        return result_image

def final_stitch(image1, image2, lowe_ratio = 0.75 , max_threshold = 4.0 , match_status = False):
  (Keypoints1, features1) = detect_feature_keypoints(image1)
  (Keypoints2, features2) = detect_feature_keypoints(image2)

  values = match_keypoints(Keypoints1, Keypoints2,features1, features2, lowe_ratio, max_threshold)
  if values is None:
    return None
  
  (matches, Homography, status) = values
  result = get_warp_perspective(image1,image2,Homography)
  result[0:image2.shape[0], 0:image2.shape[1]] = image2

  if match_status:
    vis = draw_matches(image1, image2, Keypoints1, Keypoints2, matches, status)

  return (result, vis)

pic1 = cv2.imread('NY1.png')
pic2 = cv2.imread('NY2.png')

pic1.shape

pic2.shape

pic1 = cv2.resize(pic1, (844,1129))

(result, matched_points) = final_stitch(pic1, pic2, match_status=True)

cv2_imshow(matched_points)
cv2_imshow(result)